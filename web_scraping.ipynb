{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a70eb8",
   "metadata": {},
   "source": [
    "# Web Scraping with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93783f0",
   "metadata": {},
   "source": [
    "## Pipe Line\n",
    "##### Setup\n",
    "* Define Objective\n",
    "* Identify Sources\n",
    "##### Acquisition\n",
    "* Access Raw Data\n",
    "* Parse & Extract\n",
    "##### Processing\n",
    "* Analyze --> Learn --> -- Wrangle --> Explore --> Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe2e6e",
   "metadata": {},
   "source": [
    "## Hyper Text Markup Language (HTML)\n",
    "### HTML Tags and Attributes\n",
    "```\n",
    "<tag-name attrib-name='attrib info'>\n",
    "    ... element contents ...\n",
    "</tag-name>\n",
    "```\n",
    "\n",
    "#### \"div\" tag\n",
    "```\n",
    "<div id=\"unique-id\" class=\"some class\">\n",
    "    ... div element contents ...\n",
    "</div>\n",
    "```\n",
    "* id should be unique\n",
    "* class attribute doesn't need to be unique\n",
    "\n",
    "#### \"a\" tag - for linking\n",
    "```\n",
    "<a href=\"https://www.datacamp.com\">\n",
    "    This text links to DataCamp!\n",
    "</a>\n",
    "```\n",
    "* a tags are for hyperlinks\n",
    "* href attribute tells what link to go to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b4d8b",
   "metadata": {},
   "source": [
    "## XPath\n",
    "`xpath = '/html/body/div[2]'`\n",
    "\n",
    "##### Simple XPath\n",
    "* Single forward-slash `/` used to move forward one generation\n",
    "* tag-name between slashes give direction to which element(s)\n",
    "* Brackets `[]` after a tag name tell us which of the selected siblings to choose\n",
    "\n",
    "##### Double Feature\n",
    "* Direct to all `table` elements within the entire HTML code:\n",
    "`xpath = '//table'`\n",
    "* Direct to all table elements which are descendants of\n",
    "2nd div child of the body element: `/html/body/div[2]//table`\n",
    "\n",
    "### XPaths and Selectors\n",
    "\n",
    "#### XPath Navigation\n",
    "`xpath = '/html/body'`\n",
    "\n",
    "`xpath = '/html/[1]/body[1]`\n",
    "\n",
    "#### The wild card\n",
    "* The asterisk `*` is the \"wildcard\"\n",
    "* It ignores the tag type\n",
    "\n",
    "`xpath = '/html/body/*'`\n",
    "\n",
    "#### Off the Beaten XPath\n",
    "\n",
    "##### (At)tribute\n",
    "* `@` represents \"attribute\"\n",
    "* `@class`, `@id`, `@href`\n",
    "\n",
    "`xpath = '//p[@class=\"class-1\"]'`\n",
    "\n",
    "`xpath = '//*[@id=\"uid\"]'`\n",
    "\n",
    "`xpath = '//div[@id=\"uid\"]/p[2]'`\n",
    "\n",
    "##### Content with Contains\n",
    "* `xpath` `contains` notation:\n",
    "\n",
    "`cotains(@attri-name, \"string-expr\")`\n",
    "\n",
    "`xpath = '//*[contains(@class, \"class-1\")]'`\n",
    "\n",
    "`xpath = '//*[@class=\"class-1\"]'`\n",
    "\n",
    "`xpath = '/html/body/div/p[2]/@class'`\n",
    "\n",
    "`xpath = '//a[contains(@class, \"package-snippet\")]/@href`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81705571",
   "metadata": {},
   "source": [
    "## Selector Objects\n",
    "\n",
    "##### Setting up a `Selector`\n",
    "```\n",
    "from scrapy import Selector\n",
    "\n",
    "html = '''<html> .... </html>'''\n",
    "sel = Selector(text=html)\n",
    "```\n",
    "* Created a `scrapy` `Selector` object using a string the html code\n",
    "* The `selector` `sel` has selected the entire html document\n",
    "* Use `xpath` call within a `Selector` to create a `new Selectors` of specific pieces of html code\n",
    "* The return is a `SelectorList` of `Selector` object\n",
    "\n",
    "`sel.xpath(\"//p\")`\n",
    "\n",
    "##### Extracting data from a `SelectorList`\n",
    "* Use the `extract()` method: `sel.xpath(\"//p\").extract()`\n",
    "* Use `extract_first()` method to get the first element from the list\n",
    "`sel.xpath(\"//p\").extract_first()`\n",
    "\n",
    "##### Extracting data from a `Selector`\n",
    "```\n",
    "ps = sel.xpath('//p')\n",
    "second_p = ps[1]\n",
    "second_p.extract\n",
    "```\n",
    "\n",
    "##### XPath chaining\n",
    "```\n",
    "sel.xpath('/html/body/div[2]')\n",
    "#OR\n",
    "sel.xpath('/html').xpath('./body/div[2]')\n",
    "#OR\n",
    "sel.xpath('/html').xpath('./body').xpath('./div[2]')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd36003",
   "metadata": {},
   "source": [
    "### HTML text to Selector\n",
    "```\n",
    "from scrapy import Selector\n",
    "import requests\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Web_scraping'\n",
    "html = reqeusts.get(url).content\n",
    "sel = Selector(text=html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11cdd64",
   "metadata": {},
   "source": [
    "## CSS Locators, Chaining and Responses\n",
    "\n",
    "##### CSS Locators\n",
    "* `/` replace by `>` (except first character)\n",
    "    * XPath: `/html/body/div`\n",
    "    * CSS Locator: `html > body > div`\n",
    "* `//` replaced by a `blank space` (except first character)\n",
    "    * XPath: `//div/p[2]`\n",
    "    * CSS Locator: `div > span p`\n",
    "* `[N]` replaced by `:nth-of-type(N)`\n",
    "    * XPath: `//div/p[2]`\n",
    "    * CSS Locator: `div > p:nth-of-type(2)`\n",
    "* Example:\n",
    "```\n",
    "xpath = '/html/body//div/p[2]'\n",
    "css = 'html > body div > p:nth-of-type(2)'\n",
    "```\n",
    "\n",
    "##### Attribute in CSS\n",
    "* To find an element by class, use a period `.`\n",
    "    * ex: `p.class-1` selects all paragraph elements belonging to clas-1\n",
    "* To find an element by id, use a pound sign `#`\n",
    "    * ex: `div#uid` selects the div element with id equal to uid\n",
    "* `div#uid > p.class1` - select paragraph elements within class1\n",
    "* `.class1` - select all elements whose class attribute belongs to class1\n",
    "\n",
    "##### Selectors with CSS\n",
    "```\n",
    "from scrapy import Selector\n",
    "\n",
    "html = '''<html> ...... </html>'''\n",
    "sel = Selector(text = html)\n",
    "\n",
    "sel.css(\"div > p\")\n",
    "\n",
    "sel.css(\"div > p\").extract()\n",
    "```\n",
    "\n",
    "##### CSS Wildcard\n",
    "* `*` asterics - ignore tag type\n",
    "* `*` selects all elements in HTML document\n",
    "* `*.class-1` or `.class-1` selects all elements belong to class-1\n",
    "* `*#uid` or `#uid` selects the element with id attribute equal to id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b252d",
   "metadata": {},
   "source": [
    "#### Attribute and Text Selection\n",
    "* Using XPath: `<xpath-to-element>/@attr-name`\n",
    "    * `xpath = '//div[@id=\"uid\"]/a/@href'`\n",
    "* Using CSS Locator: `<css-to-element> :: attr(attr-name)`\n",
    "    * `css_locator = 'div#uid > a :: attr(href)'`\n",
    "\n",
    "##### Text Extraction - text() method\n",
    "* extracts all texts within the element not within future generation\n",
    "    * xpath: `sel.xpath('//p[@id=\"p-example\"]/text()').extract()`\n",
    "    * css: `sel.css('p#p-example:: text').extract()`\n",
    "* extracts all texts within element and in future generation\n",
    "    * xpath: `sel.xpath('//p[@id=\"p-example\"]//text()').extract()`\n",
    "    * css: `sel.css('p#p-example :: text').extract()`\n",
    "    \n",
    "#### Response\n",
    "* Response has all tools with Selectors\n",
    "    * xpath and css methods\n",
    "    * extract and extract_first methods\n",
    "* Response also keeps track of the URL where HTML code was loaded from\n",
    "* Response helps us move from one site to another, so that we can \"crawl\" the web while srapping\n",
    "* Example:\n",
    "    * xpath: `response.xpath('//div/span[@class=\"bio\"]')`\n",
    "    * css: `response.css('div > span.bio')`\n",
    "    * chaining: `response.xpath('//div').css('span.bio')`\n",
    "    * data extraction:\n",
    "        * `response.xpath('//div').css('span.bio').extract()`\n",
    "        * `response.xpath('//div').css('span.bio').extract_first()`\n",
    "* `response` keeps track of the URL within the response `url` variable `response.url`\n",
    "* `response` lets us \"follow\" a new link with the `follow()` method `response.follow(next-url)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba918e",
   "metadata": {},
   "source": [
    "## Scrapping\n",
    "```\n",
    "https://www.datacamp.com/courses/all\n",
    "course_divs = response.css('div.course-block')\n",
    "print(len(course-divs))\n",
    "```\n",
    "\n",
    "##### Inspecting course-block\n",
    "```\n",
    "first_div = course_divs[0]\n",
    "children = first_div.xpath('./*')\n",
    "print(len(children))\n",
    "```\n",
    "\n",
    "```\n",
    "# first child\n",
    "first_child = children[0]\n",
    "print(first_child.extract())\n",
    "\n",
    "# second child\n",
    "second_child = children[1]\n",
    "print(second_child.extract())\n",
    "\n",
    "# third child\n",
    "third_child = children[2]\n",
    "print(third_child.extract())\n",
    "```\n",
    "\n",
    "##### Listful\n",
    "* `css: links = response.css('div.course-block > a::attr(href)').extract()`\n",
    "\n",
    "```\n",
    "# step1: course blocks\n",
    "course_divs = response.css('div.course-block')\n",
    "\n",
    "# step2: hyperlink elements\n",
    "hrefs = course_divs.xpath('./a/@href')\n",
    "\n",
    "# step3: extract the links\n",
    "links = hrefs.extract()\n",
    "for l in links:\n",
    "    print(l)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244904a",
   "metadata": {},
   "source": [
    "## Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df563e0",
   "metadata": {},
   "source": [
    "#### A Classy Spider\n",
    "```\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SpiderClassName(scrapy.Spider):\n",
    "    name = \"spider_name\"\n",
    "    # the code for your spider\n",
    "    ...\n",
    "# Running the spider\n",
    "process = CrawlerProcess()  # initiate a Crawler Process\n",
    "process.crawl(SpiderClassName)  # tell teh process which spider to use\n",
    "process.start()  # start the crawling process\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176404d",
   "metadata": {},
   "source": [
    "#### Weaving the Web\n",
    "```\n",
    "class DCspider(scrapy.Spider)\n",
    "    name = 'dc_spider'\n",
    "    def start_requests(self):\n",
    "        urls = ['https://www.datacamp.com/courses/all']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url = url, callback = self.parse)\n",
    "    def parse(self, response):\n",
    "        # simple example: write out the html\n",
    "        html_file = 'DC_courses.html'\n",
    "        with open(html_file, 'wb') as fout:\n",
    "            fout.write(response.body)\n",
    "```\n",
    "* Need to have a function called start-requests\n",
    "* Need to have at least one parser function to handle the HTML code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5c336",
   "metadata": {},
   "source": [
    "#### Start Requests - Request for Service\n",
    "```\n",
    "def start_request(self):\n",
    "    urls = ['https://www.datacamp.com/courses/all']\n",
    "    for url in urls:\n",
    "        yield scrapy.Request(url = url, callback = self.parse)\n",
    "```\n",
    "```\n",
    "def start_requests(self):\n",
    "    url = 'htpps://www.datacamp.com/courses/all'\n",
    "    yield scrapy.Request(url = url, callback = self.parse)\n",
    "```\n",
    "* scrapy.Request will fill in a response variable\n",
    "* url argument tells which site to scrape\n",
    "* callback argument tells where to send the response variable for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de0299",
   "metadata": {},
   "source": [
    "#### Parse and Crawl\n",
    "```\n",
    "def parse(self, response):\n",
    "    # input passing code with response that you already know!\n",
    "    # output to a file\n",
    "    # crawl the web\n",
    "\n",
    "def parse(self.response):\n",
    "    links = response.css('div.course-block > a::attr(href)').extract()\n",
    "    filepath = 'DC_links.csv'\n",
    "    with open (filepath, 'w') as f:\n",
    "        f.writelines([link + '/n' for link in links])\n",
    "\n",
    "def parse(self, response):\n",
    "    links = response.css('div.course-block > a::attr(href)').extract()\n",
    "    for link in links:\n",
    "        yield response.follow(url = link, callback = self.parse2)\n",
    "\n",
    "def parse2(self, response):\n",
    "# parse the course sites here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7af6b5",
   "metadata": {},
   "source": [
    "#### Inspecting Elements\n",
    "```\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        url = 'https://www.datacamp.com/courses/all'\n",
    "        yield scrapy.Request(url = url, callback = self.parse_front)\n",
    "        \n",
    "    def parse_front(self, response):\n",
    "        # code to parse the front courses page\n",
    "    \n",
    "    def parse_pages(self, response):\n",
    "        # code to parse course pages\n",
    "        # fill in dc_dict here\n",
    "\n",
    "dc_dict = dict()\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawler(DC_Chapter_Spider)\n",
    "process.start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345987e",
   "metadata": {},
   "source": [
    "#### Parsing Front Page\n",
    "```\n",
    "def parse_front(self, response):\n",
    "    # Narrow in on the course blocks\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    \n",
    "    # Direct to the course links\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    \n",
    "    # Extract the links (as a list of strings)\n",
    "    links_to_follow = course_links.extract()\n",
    "    \n",
    "    # Follow the links the next parser\n",
    "    for url in links_to_follow:\n",
    "        yield response.follow(url = url, callback = self.parse_pages)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c9deb",
   "metadata": {},
   "source": [
    "#### Parsing the Course Pages\n",
    "```\n",
    "def pages_pages(self, response):\n",
    "    # Direct to the course title next\n",
    "    crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "    \n",
    "    # Extract and clean the course title text\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    \n",
    "    # Direct to the chapter titles text\n",
    "    ch_titles = response.css('h4.chapter--title::text')\n",
    "    \n",
    "    # Extract and clean the chapter titles text\n",
    "    cht_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    \n",
    "    # Store this in a dictionary\n",
    "    dc_dict[crs_title_ext] = ch_titles_ext\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7c9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
